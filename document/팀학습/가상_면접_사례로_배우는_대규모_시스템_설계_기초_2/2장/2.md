# 2장 주변 친구

근접성 서비스의 경우 사업장 주소는 정적이지만, 주변 친구 위치는 자주 바뀔 수 있기 때문이다.  

## 1단계: 문제 이해 및 설계 범위 확정

- 지리적으로 5마일 내에 있어야 하며, 이 수치는 설정 가능해야 함
- 두 사용자의 직선거리로만 가정
- 전체 10억명 중에 10% 1억명이 사용
- 사용자의 이동 이력은 보관해야 함
- 10분 이상 비활성 상태이면, 친구 목록에서 사라지도록 함
- GDPR, CCPA 같은 개인정보보호법은 생각하지 않기로 함

### 기능 요구사항

### 비기능 요구사항

- 낮은 지연 시간: 친구 위치가 빠르게 반영되야 한다.
- 안정성
- 결과적 일관성: 위치 데이터를 저장하기 위해 강한 일관성을 지원하는 데이터 저장소를 사용할 필요는 없다.

### 개략적 규모 추정


## 2단계: 개략적 설계안 제시 및 동의 구하기

위치 정보를 모든 친구에게 전송해야 하는 요구사항 때문에 HTTP 프로토콜을 사용하지 못 할 수 있다.  

### 개략적 설계안

이론적으로 사용자 근처의 모든 친구의 단말과 연결하는 P2P 방식을 생각할 수 있지만, 모바일 단말은 통신 상태가 좋지 않고, 전력도 충분하지 않다.  
공용 백엔드 서버가 중계해줘야 한다.  
동시 접속자가 천만명에 30초마다 갱신했을 때 초당 334,000번의 위치 정보 갱신을 해야하며, 큰 규모에 대응 할 수 없다.  

### 설계안

#### 로드밸런서

#### RESTful API

사용자와 친구 관리, 인증 및 기타 기능

#### 웹소켓 서버

친구 위치 정보 변경을 거의 실시간에 가깝게 처리  
각 클라이언트는 한 대의 웹소켓 연결을 지속적으로 유지한다.  

#### 레디스 위치 정보 캐시

레디스는 활성 상태 사용자의 가장 최근 위치 정보를 개시하는 데 사용한다.  
TTL 설정으로 캐시의 생명주기를 관리  

#### 사용자 데이터베이스

> Neo4j  
> https://www.en-core.com/kor/board/notice?viewMode=view&ca=&sel_search=&txt_search=&page=1&idx=98

#### 위치 이동 이력 데이터베이스

#### 레디스 펍/섭 서버

> 사용자 1이 발행하는 토픽을 구독하는 친구들은 동적으로 변할탠데, 이것을 어떻게 제어하는가?  
> 레디스 내에 엄청 많은 토픽이 필요할탠데, 어떻게 유지되는 것인가?  

> 채널톡 실시간 채팅 서버 개선 여정 - 1편 : 레디스의 'Pub/Sub'  
> 이번 실험의 주 목적은 레디스의 부하를 줄이는 것이었습니다. 하지만, 오히려 더 늘었던 것에는 레디스 Pub/Sub시스템 자체가 많은 PSUBSCRIBE 를 감당하기 어려운 구조로 되어있기 때문이었죠.  
> 그렇다고 개선된 어댑터가 모든 곳에서 성능 저하를 보여준 건 아니었습니다. 레디스에 부하를 더 줄지언정 소켓 서버가 필요한 메세지만을 전달받을 수 있도록 하여 오히려 소켓 서버의 부하는 줄일 수 있었습니다.  
> 이번 실험의 핵심은 레디스의 부하를 줄이는 것이기에 가설 자체는 틀렸지만, 소켓 서버와 레디스는 서로 상충관계에 놓여 있기에 소켓 서버에 더 많은 부하를 주는 방식이 필요하다면 충분히 고려해 볼 만한 방법이 아닐지 싶습니다.  
> 
> https://channel.io/ko/blog/real-time-chat-server-1-redis-pub-sub

#### 주기적 위치 갱신

한 사용자당 편균 400명의 친구가 있으며 10% 가량이 주변에서 온라인 상태로 가정하였으므로, 위치가 바뀔 때마다 40건씩 위치 정보 전송이 발생한다.  


### API 설계

#### 웹소켓

1. [서버 API] 주기적인 취지 정보 갱신
2. [클라이언트 API] 클라이언트가 갱신된 친구 위치를 수신하는 데 사용할 API
3. [서버 API] 웹소켓 초기화 API
4. [클라이언트 API] 새 친구 구독 API
5. [클라이언트 API] 구독 해지 API

#### HTTP 요청

1. 친구 추가/삭제
2. 사용자 정보 갱신


### 데이터 모델

#### 위치 정보 캐시

사용자ID (키) : {위도, 경도, 시각} (값)

##### 위치 정보 저장에 데이터베이스를 사용하지 않는 이유는?

- 현재 위치만 필요함
- 읽기 / 쓰기 연산이 빠름
- TTL 설정으로 사용하지 않는 값을 자동으로 제거 가능
- 레디스 서버에 장애가 발생하여, 위치 변경 내역을 놓치더라도 서비스 특성상 수용 가능

##### 위치 이동 이력 데이터베이스

카산드라는 막대한 쓰기 연산 부하를 감당할 수 있고, 수평적 규모 확장이 가능  
관계형 데이터베이스를 사용할 경우 샤딩이 필요하며, 사용자ID를 기준 삼는 샤딩이 가장 기본이다.  

> 샤딩 정책은 유명인 문제를 고민해야 함  


## 3단계: 상세 설계

### 중요 구성요소별 규모 확장성

#### API 서버

> 오토스케일링

#### 웹소켓 서버

> 오토스케일링
> 
> Spring-boot graceful shutdown  
> https://velog.io/@dongvelop/Springboot-Graceful-Shutdown  

#### 클라이언트 초기화

모바일 클라이언트는 기동되면 웹소켓 클러스터 내의 서버 가운데 하나와 지속성 웹소켓 연결을 맺는다.  
웹소켓 연결이 초기화되면 클라이언트는 해당 모바일 단말의 위치, 즉 해당 단말을 이용 중인 사용자의 위치 정보를 전송한다.  

웹소켓 연결 핸들러 (백엔드) 작업 수행 순서
1. 사용자의 위치 정보 캐시 갱신
2. 위치 정보를 연결 핸들러 내의 변수에 저장
3. 사용자 데이터베이스에서 친구 정보 조회
4. 위치 정보 캐시에 모든 친구의 위치 정보 조회
5. 친구와 사용자 사이의 거리를 계산하고, 적합한 대상만 웹소켓 연결을 통해 클라이언트에 반환
6. 각 친구의 레디스 펍/섭 채널 구독
7. 사용자의 위치를 레디스 펍/섭 채널을 통해 모든 친구에게 전송

#### 사용자 데이터베이스

- 사용자 정보
- 친구 관계 정보

이번 장에서 다루는 설계안의 규모를 감안하면 한 대의 관계형 데이터베이스 서버로는 감당할 수 없음
관계형 데이터베이스인 경우 사용자ID로 샤딩을 해야 함

실제 운영환경에서는 웹 소켓 서버가 친구 관계 DB를 직접 조회하는 대신 내부 API를 사용해야 함
그렇더라도 시스템 성능에는 큰 차이가 없을 것이다.  

#### 위치 정보 캐시

위치 정보 보관에 100바이트가 필요하다고 가정할 경우, 첫만 명의 활성 사용자가 30초마다 변경된 위치 정보를 전송한다면, 초당 334K 연산이 필요함  
최고 사양의 서버라도 부담되는 수치라서, 사용자 ID 기준으로 샤딩을 함으로 해결 가능  
가용성을 높이려면 위치 정보를 대기 노드에도 복제해두면 장애 발생 시 빠르게 대처 가능  

> 레디스에 개인화 정보(큰 문자열)을 캐싱하다가 장애가 발생한 사례를 봄

#### 레디스 펍/섭 서버

새 처널은 구독하려는 채널이 없을 때 생성한다.  
채널 하나를 유지하기 위해서는 구독자 관계를 추적하기 위한 해시 테이블과 연결 리스트가 필요  

아키텍처를 단순하게 만들 수 있다면 더 많은 메모리를 투입할 만한 가치는 충분하다.  

#### 얼마나 많은 레디스 펍/섭 서버가 필요한가?

##### 메모리 사용량

- 필요한 채널의 수: 10억 사용자 * 10% = 1억 개  
- 모든 채널 저장 용량: 1억 * 20바이트 * 100 * 10^9 = 200GB

##### CPU 사용량

펍/섭 서버가 구독자에게 전송해야 하는 위치 정보 업데이트의 양은 초당 1400만 건  
서버 한 대당 감당 가능한 구독자의 수는 100,000인 경우, 레디스 서버 140대가 필요함  

**레디스 펍/섭 서버의 병목은 메모리가 아니라 CPU 사용량이다.**  
분산 레디스 펍/섭 서버가 필요하다.  

#### 분산 레디스 펍/섭 서버 클러스터

메세지를 발행할 사용자 ID를 기준으로 펍/섭 서버들을 샤딩  
etcd, 주키버와 같은 서비스 탐색 컴포넌트로 샤딩 문제 해결  

채널의 해시값을 해시링에 위치시키고, 적합한 레디스 펍/섭 서버에 할당함  
웹소켓 서버는 채널의 해시값으로 메세지를 발행할 펍/섭 서버를 선정하고, 위치 정보 변경 내역을 발행함  

성능 효율을 높이고 싶다면, 해시 링 사본을 웹 소켓 서버가 갖고 있도록 하여 펍/섭 서버를 선정하는 단계를 건너 뛸 수 있음  
다만, 웹 소켓의 해시 링 사본은 원본과 반드시 동일해야 함  

#### 레디스 펍/섭 서버 클러스터의 규모 확장 고려사항

1. 펍/섭 채널에 전송되는 메세지는 메모리나 디스크에 지속적으로 보관되지 않는다.  
2. 펍/섭 서버는 채널에 대한 상태 정보(각 채널의 구독자 목록)를 보관한다.  

2번 이유로 레디스 펍/섭 서버 클러스터는 유상태 서버 클러스터로 취급해야 한다.  
유상태 서버 클러스터는 혼잡 시간대 트래픽을 무리 없이 감당하고 불필요한 크기 변화를 피할 수 있도록 오버 프로비저닝하는 것이 보통이다.  

클러스터의 크기를 조정하면 해시 링의 정보가 변경되고, 웹소켓 서버는 변경된 내용을 반드시 갱신해야 한다.  
엄청난 재구독 요청 발생하고, 처리 과정에서 클라이언트가 보내는 위치 정보 변경이 누락될 수 있다.  
클러스터 크기 조정은 시스템 부하가 가장 낮은 시간에 해야 한다.  

- 새로운 해시 링 크기를 계산하고, 크기가 늘어날 경우 새 서버를 준비한다.  
- 해시 링의 키에 매달린 값을 새로운 내용으로 갱신한다.  
- 대시 보드를 모니터링 한다. 

#### 운영 고려사항

장애가 발생한 서버를 신규 서버로 교체  
교체 되는 서버의 채널만 신규 서버로 구독 되도록 다시 설정하면 됨  

#### 친구 추가/삭제

친구가 추가/삭제되면 클라이언트가 웹 소켓 서버에 요청  
웹 소켓 서버는 친구의 펍/섭 채널을 구독하거나 구독을 취소  

#### 친구가 많은 사용자

친구 관계는 양방향 모델로 가정하고, 상한 값(페이스북은 5,000)이 존재할 경우, 핫스팟 문제는 발생하지 않을 것  

#### 주변의 임의 사용자

지오해시에 따라 구축된 펍/섭 채널 풀을 둔다.  

격자 경계 부근에 있는 사용자를 잘 처리하기 위해 모든 클라이언트는 사용자가 위치한 지오해시뿐 아니라 주변 지오해시 격자를 담당하는 채널도 구독한다.  

#### 레디스 펍/섭 외의 대안

얼랭은 고돌 분산된 병렬 애플리케이션을 위해 고안된 프로그래밍 언어이자 런타임 환경이다.  

> 금융업계에서 얼랭(Erlang) 언어가 잊히지 않는 이유  
> 한 번에 수천 개의 상호 작용이 발생하는 금융 서비스 산업에서 유용할 수 있다.  
> https://www.cwn.kr/news/articleView.html?idxno=7725

## 4단계: 마무리



