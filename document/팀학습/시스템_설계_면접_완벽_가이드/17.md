# 17 판매량 기준 아마존 상위 10개 제품 대시보드 설계

## 17.1 요구사항

- 동점은 어떻게 처리하나?
- 원하는 정확도와 일관성/지연 시간은 어느 정도인가?
- 상위 K 목록에 판매 수를 표시해야 하나, 아니면 제품 판매 순위만 표시하면 되나?
- 판매 후 발생하는 이벤트를 고려해야 하나?

실시간으로 정확한 판매량과 순위를 계산하는 것은 리소스가 많이 필요할 것이다.  

정확도를 낮추는 대신 더 높은 확장성, 더 낮은 비용, 더 낮은 복잡성, 더 나은 유지보수성을 얻는 것을 고려할 수 있다.  

하루에 100억 건 대량의 판매 거래 트래픽 판매 이벤트가 있다고 가정한다.  
이벤트당 1KB라면 쓰기 속도는 하루 10TB다.  
상위 K 문제 대시보드는 직원만 볼 수 있으므로 요청 비율이 낮을 것이다.  


## 17.2 초기 구상

첫 번째 생각은 이벤트를 HDFS나 일래스틱서치와 같은 분산 저장소 솔루션에 기록하고, 특정 기간 내 상위 K 제품 목록을 계산해야 할 때 맵리듀스나 스파크, 또는 일래스틱서치 쿼리를 실행하는 것일 수 있다.  
그러나 이 접근 방식은 계산 집약적이며 시간이 너무 오래 걸릴 수 있다.  
이 목록 생성을 제외하고는 판매 이벤트 로그를 장기간 보관할 뚜렷한 이유가 없다면 이 목적만을 위해 이러한 로그를 몇 달이나 몇 년 동안 저장하는 것은 낭비다.  

이러한 상위 K 목록을 계산하기 전에 데이터를 전처리해야 한다.  
주기적으로 집계를 수행하고 제품 판매량을 계산하며, 시간, 일, 주, 월, 연 단위로 버킷팅 해야 한다.  

버킷팅: 데이터를 특정 기준에 따라 여러 그룹(버킷)으로 나누어 구성하는 데이터 구조화와 분류 기법이다.  

판매가 매우 불균등할 수 있으므로 버킷을 저장해야 한다.  


## 17.3 초기 고수준 아키텍처

람다 아키텍처는 배치와 스트리밍 방법을 모두 사용해 대량의 데이터를 처리하는 접근 방식이다.  
람다 아키텍처는 두 개의 병렬 데이터 처리 파이프라인과 이 두 파이프라인의 결과를 결합하는 서빙 계층으로 구성된다.  

EDA 접근 방식에 따라 판매 백엔드 서비스는 카프카 토픽으로 이벤트를 보내고, 이는 상위 K 대시보드와 같은 모든 다운스트림 분석에 사용될 수 있다.  


## 17.4 집계 서비스

### 17.4.1 제품 ID별 집계


### 17.4.2 호스트 ID와 제품 ID 매칭

> 집계 서비스를 제품 ID별로 분할할 수 있다는 내용이 왜 나온건지 이해하지 못 함  
> 제품 ID별로 분할 했을 때 이점이 무엇인지?  
> 제품 ID별로 불균형(핫 키)이 발생할 수 있는게 아닌지?  


### 17.4.3 타임스탬프 저장

정확한 타임스탬프 저장이 필요하다면 이 저장은 분석이나 상위 K 문제 서비스가 아닌 판매 서비스에서 처리해야 한다.  
책임의 분리를 유지해야 한다.  
다른 서비스와 무관하게 파이프라인을 개발하고 폐기할 수 있는 완전한 자유가 있어야 한다.  


### 17.4.4 호스트의 집계 프로세스

집계 호스트는 제품 ID를 키로, 수량을 값으로 하는 해시 테이블을 포함한다.  
또한 소비스하는 카프카 토픽에 체크포인팅을 수행하며, 체크포인트를 레디스에 쓴다.  

> outbox 패턴를 의미하는건지?

집계 호스트는 설정된 주기나 메모리가 부족할 때 중 더 빠른 시점에 해시 테이블을 플러시할 수 있다.  

> 애플리케이션 메모리에 해시 테이블로 쌓고 있다가, 특정 시점에 해시 테이블을 카프카에 메시지를 발행하는 것으로 이해함  
> 컨슈머가 메시지를 읽어서 대량의 데이터를 저장소에 저장하는 방식인 듯  

진행 중 실패해서 반복해야 할 때를 대비해 실패 복구 메커니즘 자체도 멱등성을 가져야 한다.  

내결함성을 고려해야 한다.  
모든 쓰기 작업은 실패할 수 있다.  

여러 서비스에 이벤트를 써야 하는 상황에서 일관성 없음을 방지하는 방법은 무엇인가?  

- 체크포인트
- 주기적 감사는 숫자가 맞지 않으면 일관되지 않은 결과를 버리고 관련 데이터를 재처리한다.
- 2PC, 사가 패턴, CDC나 트랜잭션 감독자와 같은 분산 트랜잭션 기술을 사용한다.


## 17.5 배치 파이프라인

시간별, 일별, 주별, 그리고 월별과 연도별로 롤업한다.  

시간별 롤업 작업은 수십억 건의 판매 이벤트를 처리해야 할 수 있으므로 하이브 쿼리를 사용해 HDFS에서 읽은 다음 결과 수를 SQL batch_table에 쓸 수 있다.  

시간별보다 큰 모든 롤업에서는 종속 롤업 실행이 성공적으로 완료됐는지 확인하는 작업이 필요하다.  

> speed_table GPT 설명:  
> speed_table은 원본 대용량 테이블을 그대로 쓰면 성능이 안 나올 때, 조인이나 조회 성능을 높이기 위해 별도로 축소·캐싱한 보조 테이블을 말합니다.  
> 자주 쓰는 키/상태/메타 데이터를 담아두고, 배치 집계 시 이 테이블과 조인해 처리 속도를 높이는 용도로 쓰입니다.  


## 17.6 스트리밍 파이프라인

배치 작업은 완료하는 데 많은 시간이 걸릴 수 있어 모든 간격의 롤업에 영향을 미칠 수 있다.  

### 17.6.1 단일 호스트의 해시 테이블과 최대 힙


### 17.6.2 여러 호스트로의 수평적 확장과 다중 계층 집계

최종 해시 테이블 호스트로의 트래픽이 너무 높다면 스트리밍 파이프라인에 다중 계층 접근 방식을 사용할 수 있다.  
이 접근 방식에서는 첫 번째 계층의 호스트와 최종 해시 테이블 호스트 사이에 더 많은 계층을 삽입해 어떤 호스트도 처리할 수 있는 것보다 많은 트래픽을 받지 않게 한다.  

> 첫 회사때 광고 노출수, 클릭수 집계하는 방식과 유사  
> 수집기는 nginx 로그 '2025-09-22 21:32:05, id:12345'를 로그 파일에 기록  
> 수집기는 크론탭이 있어서 10분마다 로그 파일의 데이터를 읽어서 10분 단위 파일 2025-09-22T21:30.json으로 적재  
> 처리기는 10분 단위 파일을 읽어서 데이터를 파싱하여 DB에 적재  
 
