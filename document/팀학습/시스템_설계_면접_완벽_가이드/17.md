# 17 판매량 기준 아마존 상위 10개 제품 대시보드 설계

## 17.1 요구사항

- 동점은 어떻게 처리하나?
- 원하는 정확도와 일관성/지연 시간은 어느 정도인가?
- 상위 K 목록에 판매 수를 표시해야 하나, 아니면 제품 판매 순위만 표시하면 되나?
- 판매 후 발생하는 이벤트를 고려해야 하나?

실시간으로 정확한 판매량과 순위를 계산하는 것은 리소스가 많이 필요할 것이다.  

정확도를 낮추는 대신 더 높은 확장성, 더 낮은 비용, 더 낮은 복잡성, 더 나은 유지보수성을 얻는 것을 고려할 수 있다.  

하루에 100억 건 대량의 판매 거래 트래픽 판매 이벤트가 있다고 가정한다.  
이벤트당 1KB라면 쓰기 속도는 하루 10TB다.  
상위 K 문제 대시보드는 직원만 볼 수 있으므로 요청 비율이 낮을 것이다.  


## 17.2 초기 구상

첫 번째 생각은 이벤트를 HDFS나 일래스틱서치와 같은 분산 저장소 솔루션에 기록하고, 특정 기간 내 상위 K 제품 목록을 계산해야 할 때 맵리듀스나 스파크, 또는 일래스틱서치 쿼리를 실행하는 것일 수 있다.  
그러나 이 접근 방식은 계산 집약적이며 시간이 너무 오래 걸릴 수 있다.  
이 목록 생성을 제외하고는 판매 이벤트 로그를 장기간 보관할 뚜렷한 이유가 없다면 이 목적만을 위해 이러한 로그를 몇 달이나 몇 년 동안 저장하는 것은 낭비다.  

이러한 상위 K 목록을 계산하기 전에 데이터를 전처리해야 한다.  
주기적으로 집계를 수행하고 제품 판매량을 계산하며, 시간, 일, 주, 월, 연 단위로 버킷팅 해야 한다.  

버킷팅: 데이터를 특정 기준에 따라 여러 그룹(버킷)으로 나누어 구성하는 데이터 구조화와 분류 기법이다.  

판매가 매우 불균등할 수 있으므로 버킷을 저장해야 한다.  


## 17.3 초기 고수준 아키텍처

람다 아키텍처는 배치와 스트리밍 방법을 모두 사용해 대량의 데이터를 처리하는 접근 방식이다.  
람다 아키텍처는 두 개의 병렬 데이터 처리 파이프라인과 이 두 파이프라인의 결과를 결합하는 서빙 계층으로 구성된다.  

EDA 접근 방식에 따라 판매 백엔드 서비스는 카프카 토픽으로 이벤트를 보내고, 이는 상위 K 대시보드와 같은 모든 다운스트림 분석에 사용될 수 있다.  


## 17.4 집계 서비스

### 17.4.1 제품 ID별 집계


### 17.4.2 호스트 ID와 제품 ID 매칭

> 집계 서비스를 제품 ID별로 분할할 수 있다는 내용이 왜 나온건지 이해하지 못 함  
> 제품 ID별로 분할 했을 때 이점이 무엇인지?  
> 제품 ID별로 불균형(핫 키)이 발생할 수 있는게 아닌지?  


### 17.4.3 타임스탬프 저장

정확한 타임스탬프 저장이 필요하다면 이 저장은 분석이나 상위 K 문제 서비스가 아닌 판매 서비스에서 처리해야 한다.  
책임의 분리를 유지해야 한다.  
다른 서비스와 무관하게 파이프라인을 개발하고 폐기할 수 있는 완전한 자유가 있어야 한다.  


### 17.4.4 호스트의 집계 프로세스

집계 호스트는 제품 ID를 키로, 수량을 값으로 하는 해시 테이블을 포함한다.  
또한 소비스하는 카프카 토픽에 체크포인팅을 수행하며, 체크포인트를 레디스에 쓴다.  

> outbox 패턴를 의미하는건지?

집계 호스트는 설정된 주기나 메모리가 부족할 때 중 더 빠른 시점에 해시 테이블을 플러시할 수 있다.  

> 애플리케이션 메모리에 해시 테이블로 쌓고 있다가, 특정 시점에 해시 테이블을 카프카에 메시지를 발행하는 것으로 이해함  
> 컨슈머가 메시지를 읽어서 대량의 데이터를 저장소에 저장하는 방식인 듯  

진행 중 실패해서 반복해야 할 때를 대비해 실패 복구 메커니즘 자체도 멱등성을 가져야 한다.  

내결함성을 고려해야 한다.  
모든 쓰기 작업은 실패할 수 있다.  

여러 서비스에 이벤트를 써야 하는 상황에서 일관성 없음을 방지하는 방법은 무엇인가?  

- 체크포인트
- 주기적 감사는 숫자가 맞지 않으면 일관되지 않은 결과를 버리고 관련 데이터를 재처리한다.
- 2PC, 사가 패턴, CDC나 트랜잭션 감독자와 같은 분산 트랜잭션 기술을 사용한다.


## 17.5 배치 파이프라인

시간별, 일별, 주별, 그리고 월별과 연도별로 롤업한다.  

시간별 롤업 작업은 수십억 건의 판매 이벤트를 처리해야 할 수 있으므로 하이브 쿼리를 사용해 HDFS에서 읽은 다음 결과 수를 SQL batch_table에 쓸 수 있다.  

시간별보다 큰 모든 롤업에서는 종속 롤업 실행이 성공적으로 완료됐는지 확인하는 작업이 필요하다.  

> speed_table GPT 설명:  
> speed_table은 원본 대용량 테이블을 그대로 쓰면 성능이 안 나올 때, 조인이나 조회 성능을 높이기 위해 별도로 축소·캐싱한 보조 테이블을 말합니다.  
> 자주 쓰는 키/상태/메타 데이터를 담아두고, 배치 집계 시 이 테이블과 조인해 처리 속도를 높이는 용도로 쓰입니다.  


## 17.6 스트리밍 파이프라인

배치 작업은 완료하는 데 많은 시간이 걸릴 수 있어 모든 간격의 롤업에 영향을 미칠 수 있다.  

### 17.6.1 단일 호스트의 해시 테이블과 최대 힙


### 17.6.2 여러 호스트로의 수평적 확장과 다중 계층 집계

최종 해시 테이블 호스트로의 트래픽이 너무 높다면 스트리밍 파이프라인에 다중 계층 접근 방식을 사용할 수 있다.  
이 접근 방식에서는 첫 번째 계층의 호스트와 최종 해시 테이블 호스트 사이에 더 많은 계층을 삽입해 어떤 호스트도 처리할 수 있는 것보다 많은 트래픽을 받지 않게 한다.  

> 첫 회사때 광고 노출수, 클릭수 집계하는 방식과 유사  
> 수집기는 nginx 로그 '2025-09-22 21:32:05, id:12345'를 로그 파일에 기록  
> 수집기는 크론탭이 있어서 10분마다 로그 파일의 데이터를 읽어서 10분 단위 파일 2025-09-22T21:30.json으로 적재  
> 처리기는 10분 단위 파일을 읽어서 데이터를 파싱하여 DB에 적재  
 

## 17.7 근사

더 낮은 지연 시간을 달성하려면 집계 서비스의 계층 수를 제한해야 할 수 있다.  

최대 힙을 별도의 호스트에 두는 이유는 클러스터를 확장할 때 새 호스트를 프로비저닝하기 쉽게 하기 위해서다.  

그러나 이 설계로 생성된 상위 K 목록은 부정확할 수 있다.  

- 호스트1: {A:7, B:6, C:5} > 상위 2개일 때 최대 힙: {A:7, B:6}
- 호스트2: {A:2, B:4, C:5} > 상위 2개일 때 최대 힙: {B:4, C:5}
- 잘못된 최대 힙: {A:7, B:10}
- 올바른 최대 힙: {B:10, C:11}


### 17.7.1 카운트-민 스케치

이전 접근 방식은 각 호스트에서 제품 수와 같은 크기의 해시 테이블에 많은 메모리가 필요하다.  
근사를 사용해 정확성과 낮은 메모리 소비를 교환하는 것을 고려할 수 있다.  

카운트-민 스케치는 적합한 근사 알고리즘이다.  

> 충돌이 가장 많이 발생한 해시 값을 역추적해서 값을 알 수 있다는 것으로 이해함  
> 5(충돌)은 A,B,C가 공통으로 갖고 있는 해시값이므로 제외를 한다면, 그 다음 많은 값은 3이며, 3이 카운트된 원인을 역추적하면 C에 도달 가능  


## 17.8 람다 아키텍처를 사용한 대시보드

P99를 1초 미만으로 달성하려면 SQL 쿼리는 순위와 수를 포함하는 단일 뷰의 간단한 SELECT 쿼리여야 하며, 이를 top_1000 뷰라고 부른다.  


## 17.9 카파 아키텍처 접근 방식

카파 아키텍처는 스트리밍 데이터를 처리하기 위한 소프트웨어 아키텍처 패턴으로, 단일 기술 스택으로 배치와 스트리밍 처리를 모두 수행한다.  
이는 들어오는 데이터를 저장하기 위해 카프카와 같은 추가 전용 불변 로그를 사용하고, 이후 스트림 처리와 사용자가 쿼리할 수 있는 데이터베이스에 저장하는 과정을 거친다.  


### 17.9.1 람다 vs 카파 아키텍처

람다 아키텍처는 복잡하다.  
배치 계층과 스트리밍 계층이 각각 자체 코드베이스의 클러스터를 필요로 하며, 이에 따른 운영 부담과 개발, 유지보수, 로깅, 모니터링, 경보에 관련된 복잡성과 비용이 발생하기 때문이다.  

카파 아키텍처는 람다 아키텍처를 단순화한 것으로, 스트리밍 계층만 있고 배치 계층은 없다.  
이는 단일 기술 스택에서 스트리밍과 배치 처리를 모두 수행하는 것과 유사하다.  
서빙 계층은 스트리밍 계층에서 계산된 데이터를 제공한다.  
성능을 위해 정확도를 트레이드오프 할 수 있다.  

> 빅 데이터 아키텍처  
> https://learn.microsoft.com/ko-kr/azure/architecture/databases/guide/big-data-architectures

하지만 이런 트레이드오프를 하지 않고 매우 정확한 데이터를 계산하게 선택할 수도 있다.  
카파 아키텍처는 배치 작업이 전혀 필요하지 않고 스트리밍이 모든 데이터 처리 작업과 요구사항을 처리할 수 있다는 주장에서 비롯됐다.  

HDFS와 같은 분산 파일 시스템을 사용하는 배치 작업은 적은 양의 데이터에서 실행하더라도 완료하는 데 최소 몇 분이 걸리는 경향이 있다.  

배치 작업 실패는 개발부터 테스트, 운영 환경에 이르는 전체 소프트웨어 개발 수명 주기 동안 실질적으로 불가피하며, 배치 작업이 실패하면 다시 실행해야 한다.  
배치 작업을 기다리는 시간을 줄이기 위한 일반적인 기법 중 하나는 이를 여러 단계로 나누는 것이다.  
개발자와 운영팀은 여전히 작업이 성공했는지 실패했는지 확인하려면 30분이나 1시간을 기다려야 한다.  

> 대량의 데이터 처리를 배치로 하던 것을 이벤트 스트리밍 방식으로 개선하는 것으로 이해함  

배치 작업에서는 하나의 버그가 전체 작업 실패로 이어질 수 있다.  
스트리밍에서는 단일 버그가 해당 특정 이벤트의 처리만 영향을 미친다.  

카파 아키텍처가 람다 아키텍처에 비해 갖는 또 다른 장점은 전자의 상대적 단순성이다.  

카파 아키텍처의 한 가지 고려사항은 카프카와 같은 이벤트 스트리밍 플랫폼에 대량의 데이터를 저장하는 것이 대용량을 위해 설계된 HDFS와 달리 비용이 많이 들고 몇 PB 이상으로 확장할 수 없다는 점이다.  
카프카는 로그 압축을 통해 무한 보존을 제공하므로 카프카의 로그 압축은 각 메시지 키의 최신 값만 유지하고, 이전 값은 삭제함으로써 저장 공간을 절약한다.  


### 17.9.2 대시보드를 위한 카파 아키텍처

심각한 버그는 많은 이벤트에 영향을 미칠 수 있으므로 오류를 로깅하고 모니터링하며 오류율을 모니터링해야 한다.  
이러한 버그를 해결하고 많은 수의 이벤트 스트리밍 파이프라인을 다시 실행하기 어려우므로, 중요 오류율을 정의하고 오류율이 이 정의된 중요 오류율을 초과하면 파이프라인을 카프카 소비자가 이벤트를 소비하고 처리하는 것을 중지할 수 있다.  


## 17.10 로깅, 모니터링, 경보


## 17.11 기타 논의 가능한 주제

판매 집계의 상당한 복잡성은 고객의 환불이나 교환 요청과 같은 분쟁이다.  

몇 년의 보증을 제공할 수 있으므로 판매 후 몇 년이 지나서 분쟁이 발생할 수 있다.  

