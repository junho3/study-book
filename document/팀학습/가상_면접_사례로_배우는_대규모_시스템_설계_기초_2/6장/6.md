# 6장 광고 클릭 이벤트 집계

> OpenRTB 스펙  
> https://iabtechlab.com/standards/openrtb/  
> https://www.iab.com/wp-content/uploads/2016/03/OpenRTB-API-Specification-Version-2-5-FINAL.pdf  
> 
> 비딩 금액  
> 사이트 성격  
> 지면 크기와 위치  
> 디바이스, OS  
> 지역정보, 사용자정보

## 1단계: 문제 이해 및 설계 범위 확정

- ad_id, (area_id or inventory_id), click_time-stamp, user_id, ip, country 수집
- 매일 10억 개의 광고 클릭이 발생하고, 광고는 200만 회 게재된다.
- 광고 클릭 이벤트 수는 매년 30%씩 증가한다고 가정한다.
- 질의 조건
  - 특정 광고에 대한 지난 M분간 클릭 이벤트 수
  - 지난 1분간 가장 많이 클릭된 광고 100개. 질의 기간과 광고 수는 변경 가능해야 함
  - ip, user_id, country 등의 속성을 기준으로 질의 결과를 필터링해야 함 
- 엣지 케이스
  - 예상보다 늦게 도착하는 이벤트
  - 중복된 이벤트
  - 시스템이 다운될 수 있으므로 시스템 복구를 고려해야 함
- RTB는 응답이 1초 이내여야 함
- 광고 클릭 집계는 몇 분 정도의 지연은 허용 가능

### 기능 요구사항

### 비기능 요구사항

- 집계 결과 정확성이 높아야 함
- 지연되거나 중복된 이벤트(어뷰징)를 처리할 수 있어야 함
- 부분적인 장애는 감내할 수 있어야 함
- 전체 처리 시간은 최대 수 분을 넘지 않아야 함

> 10분마다 데이터 집계를 했었음

### 개략적 추정

- DAU 10억 명
- 하루에 10억 건의 광고 클릭 이벤트 발생
- 광고 클릭 QPS = 10,000
- 최대 광고 클릭 QPS는 50,000 QPS로 가정
- 광고 클릭 이벤트 하나당 0.1KB의 저장 용량 필요. 
  - 일간 저장소 요구량은 0.1KB X 10억 = 200GB
  - 월간 저장소 요구량은 3TB

> 로그 처리 서버는 집계한 데이터를 압축해서 N일동안 보관 후 삭제  
> 로우 데이터는 S3에 적재 후 AWS Athena로 조회  


## 2단계: 개략적 설계안 제시 및 동의 구하기

### 질의 API 설계

#### API 1: 지난 M분간 각 ad_id에 발생한 클릭 수 집계

`GET: /v1/ads/{:ad_id}/aggregated_count?from=xxxx&to=xxxx&filter`

#### API 2: 지난 M분간 가장 많은 클릭이 발생한 상위 N개 ad_id 목록

`GET: /v1/ads/popular_ads?count=xxx&window=xxx&filter=xxxx`

### 데이터 모델

#### 원시 데이터

로그 파일에 포함된 원시 데이터

`ad001, 2021-01-01 00:00:01, user 1, 207.148.22.22, USA`

#### 집계 결과 데이터

| time             | ad_id | area_id | impression | click | price |
|------------------|-------|---------|------------|-------|-------|
| 2024.02.25 14:30 | ad001 | area001 | 100        | 5     | 100   |

> 책에서는 filter_id 컬럼을 갖고 있는데, 목적에 맞는 테이블을 구성하는게 더 낫다고 생각 함  
> 광고 보고서 페이지에서 다양한 필터 기능을 제공하지만, 실제로 사용하는 필터는 거의 고정되어 있음  

#### 비교

원시 데이터와 집계 결과 데이터 모두 저장하는 것을 추천한다.  

- 문제가 발생하면 디버깅에 활용할 수 있도록 원시 데이터도 보관하는 것이 좋다.  
- 원시 데이터는 양이 엄청나므로 직접 질의하는 것은 비효율적이다.  
- 원시 데이터는 백업 데이터로 활용되다. 오래된 원시 데이터는 cold storage로 옮기면 비용을 절감할 수 있다.   
- 집계 결과 데이터는 활성 데이터 구실을 한다.

> 원시 데이터는 json 형식의 파일로 관리 (AWS S3)  
> 원시 데이터는 사용자/광고/영역 지표 분석에 활용 됨  
> 
> 집계 데이터는 RDB에 저장하고, 목적에 따라 테이블이 다양함  
> 일별/월별 집계 테이블, 광고별/영역별 집계 테이블 등..

### 올바른 데이터베이스의 선택

올바른 데이터베이스를 선택하려면 다음과 같은 사항을 평가해 보아야 한다.  

- 데이터는 어떤 모습인가?
  - 관계형 데이터인가?
  - 문서 데이터인가?
  - 이진 대형 객체 (BLOB)인가?
- 작업 흐름이 읽기 중심인가 쓰기 중심인가? 아니면 둘 다인가?
- 트랜잭션을 지원해야 하는가?
- 질의 과정에서 SUM이나 COUNT 같은 온라인 분석 처리(OLAP) 함수를 많이 사용해야 하는가?

원시 데이터는 일상적인 작업에는 질의할 필요가 없지만, 데이터 연구 목적으로 유용하다.  
설계 범위 확정 단계에서 평균 쓰기 10,000 QPS이고, 최대 50,000QPS 이므로 쓰기 중심 시스템이다.  

ORC, Parquet, AVRO 같은 컬럼형 데이터 형식 가운데 하나를 사용하여 아마존 S3에 데이터를 저장하는 방법도 있다.  
이 구성은 많은 사람들에게 낯설 것이므로, 본 설계안에서는 카산드라를 활용한다.  

집계 데이터는 본질적으로 시계열 데이터이며 이 데이터를 처리하는 워크플로는 읽기 연산과 쓰기 연산을 둘 다 많이 사용한다.  
각 광고에 대해 매 분마다 데이터베이스에 질의를 던져 고객에게 최신 집계 결과를 제시해야 하기 때문이다.  
집계 서비스가 데이터를 매 분 집계하고 그 결과를 기록하므로 쓰기 작업도 아주 빈번하게 이루어진다.  
원시 데이터와 집계 결과 데이터를 저장하는 데는 같은 유형의 데이터베이스를 활용하는 것이 가능하다.  

### 개략적 설계안

실시간으로 빅데이터를 처리할 때 데이터는 보통 무제한으로 시스템에 흘러 들어왔다가 흘러나간다.  
입력은 원시 데이터이고, 출력은 집계 결과다.  

#### 비동기 처리

데이터를 동기식으로 처리하는 방식은 생산자와 소비자 용량이 항상 같을 수는 없으므로 좋지 않다.  
트래픽이 갑자기 증가하여 발생하는 이벤트 수가 소비자의 처리 용량을 훨씬 넘어서는 경우, 소비자는 메모리 부족 오류 등의 예기치 않은 문제를 겪게 될 수 있다.  

이 문제를 해결하는 방안은 카프카 같은 메시지 큐를 도입하여 생산자와 소비자의 결합을 끊는 것이다.  

집계 결과를 데이터베이스에 바로 기록하지 않는 이유는 정확하게 한 번(exactly once) 데이터를 처리하기 위해 카프카 같은 시스템을 두 번째 메시지 큐로 도입해야 하기 때문이다.  

### 집계 서비스

광고 클릭 이벤트를 집계하는 좋은 방안은 맵리듀스 프레임워크를 사용하는 것이다.  
DAG 모델의 핵심은 시스템을 맵/집계/리듀스 노드 등의 작은 컴퓨팅 단위로 세분화하는 것이다.  
각 노드는 한 가지 작업만 처리하며, 처리 결과를 다음 노드에 인계한다.  

#### 맵 노드

맵 노드는 데이터 출처에서 읽은 데이터를 필터링하고 변환하는 역할을 담당한다.  
입력 데이터를 정리하거나 정규화해야 하는 경우에는 맵 노드가 필요하다.  

#### 집계 노드

집계 노드는 ad_id 별 광고 클릭 이벤트 수를 매 분 메모리에서 집계한다.  

##### 리듀스 노드

리듀스 노드는 모든 '집계' 노드가 산출한 결과를 최종 결과로 축약한다.  

DAG는 맵리듀스 패러다임을 표현하기 위한 모델이다.  
빅데이터를 입력으로 받아 병렬 분산 컴퓨팅 자원을 활용하여 빅데이터를 작은 크기 데이터로 변환할 수 있도록 설계된 모델이다.  
이 모델의 중간 데이터는 메모리에 저장될 수 있으며, 노드 간 통신은 TCP로 처리할 수도 있고 공유 메모리로 처리할 수도 있다.  

##### 주요 사용 사례

###### 사례 1: 클릭 이벤트 수 집계

###### 사례 2: 가장 많이 클릭된 상위 N개 광고 반환

###### 사례 3: 데이터 필터링

> RDB에서 필터링할 컬럼에 유니크 키를 설정하는 것으로 이해 함  


## 3단계: 상세 설계

### 스트리밍 vs 일괄처리

스트림 처리는 데이터를 오는 대로 처리하고 거의 실시간으로 집계된 결과를 생성하는 데 사용한다.  
일괄 처리는 이력 데이터를 백업하기 위해 활용한다.  

일괄 및 스트리밍 처리 경로를 동시에 지원하는 시스템 아키텍처를 람다라고 부른다.  
카파 아키텍처는 일괄 처리와 스트리밍 처리 경로를 하나로 결합하여 관리해야 할 코드를 하나로 만든다.  

> 카파 아키텍처? 람다 아키텍처? 빅데이터 플랫폼 아키텍처의 미래 살펴보기  
> 람다 아키텍처: 배치 레이어(스파크잡), 스피드 레이어(카프카), 서빙 레이어(하둡)  
> 카파 아키텍처: 스피드 레이어(카프카), 서빙 레이어(하둡)  
> 스트리밍 데이터 레이크: 스피드 레이어(카프카)  
> https://youtu.be/U5G-i73Wb6U?feature=shared

#### 데이터 재계산

원시 데이터 저장소에서 데이터를 검색 후 데이터 집계 서비스를 사용하여 데이터를 재계산한다.  

#### 시간

이벤트 시각: 광고 클릭이 발생한 시각이다.  
처리 시각: 집계 서버가 클릭 이벤트를 처리한 시스템 시각이다.  

> 책에서는 이벤트 발생 시각을 클라이언트(프론트)와 집계 서버에서 처리하는 것을 소개 하고 있는데, 클릭을 받아주고 랜딩 페이지로 리다이렉트 시켜주는 서버가 별도로 있음  
> 웹페이지에서 이미지 배너 클릭 > 클릭 처리 서버 (이벤트 발생 시각 셋팅) > 랜딩 페이지로 리다이렉트  
> 따라서 악성 사용자가 타임스탬프를 고의로 조작할 수 없음  
> 
> 집계 방식과 시각은 중요함  
> 클릭수(비용)를 서로 맞춰보는데, 다 다름  

집계 범위보다 조금 더 넓게 잡는 워터마크 기술을 사용해서 늦게 들어온 데이터까지 집계할 수 있도록 한다.  
예를 들어 집계 범위가 1분인 경우, 데이터 조회 범위를 1분 30초까지 늘려서 1분 1초에 들어온 데이터를 집계할 수 있도록 한다.  

워터마크 구간이 길면 늦게 도착하는 이벤트도 포착할 수 있지만 시스템의 이벤트 처리 시간은 늘어난다.  
워터마크가 짧으면 데이터 정확도는 떨어지지만 시스템의 응답 지연은 낮아진다.  

워터마크 기법으로도 시간이 한참 흐른 후에 시스템에 도달하는 이벤트는 처리할 수 없다.  
발생할 확률이 낮은 이벤트 처리를 위해 시스템을 복잡하게 설계하면 투자 대비 효능이 떨어진다.  

> AD 마스터라고 불리는 광고 운영자들이 분 단위까지는 관심 갖진 않았고, 시 단위까지는 확인했던 것으로 기억 함  

### 집계 윈도

텀블링 윈도는 시간을 같은 크기의 겹치지 않는 구간으로 분할한다.  
슬라이딩 윈도는 데이터 스트림을 미끄러져 나아가면서 같은 시간 구간 안에 있는 이벤트를 집계한다.  
슬라이딩 윈도는 서로 겹칠 수 있다.  

### 전달 보장

- 이벤트의 중복 처리를 어떻게 피할 수 있는가?  
- 모든 이벤트의 처리를 어떻게 보장할 수 있는가?  

#### 어떤 전달 방식을 택할 것인가

데이터의 몇 퍼센트 차이가 수백만 달러 차이로 이어질 수 있다.  
따라서 '정확히 한 번' 방식을 권장한다.  

##### 데이터 중복 제거

- 클라이언트 측: 한 클라이언트가 같은 이벤트를 여러 번 보내는 경우
- 서버 장애: 집계 도중에 집계 서비스 노드에서 장애가 발생하였고 업스트림 서비스가 이벤트 메시지에 대해 응답을 받지 못 한경우, 같은 이벤트가 다시 전송되어 재차 집계될 가능성이 있다.  

HDFS / S3 같은 외부 파일 저장소에 오프셋을 기록하는 것이다.  

> 책에서 소개한 내용으로 '정확히 한 번'을 처리하기 위한 아이디어는 얻었음  

### 시스템 규모 확장

#### 메시지 큐의 규모 확장

시스템에 수백 개 카프카 컨슈머가 있는 경우에는 재조정 작업 시간이 길어져서 수 분 이상 걸리게 될 수 있다.  
더 많은 컨슈머를 추가하는 작업은 시스템 사용량이 많지 않은 시간에 실행하여 영향을 최소화하는 것이 좋다.  

> 파티션을 늘리는 것은 비용이 큰 작업이므로 파티션을 증설한 새로운 토픽으로 교체하는 방식은 어떤지?  
> 광고 수집은 반드시 실시간일 필요는 없으므로 파티션을 증설한 v2 토픽과 컨슈머를 셋팅하고, 프로듀서를 배포하는게 안전하지 않을까  

#### 브로커

- 해시 키: 같은 ad_id를 갖는 이벤트를 같은 카프카 파티션에 저장하기 위해 ad_id를 해시 키로 사용한다.  
- 파티션의 수: 파티션의 수가 변하면 같은 ad_id를 갖는 이벤트가 다른 파티션에 기록되는 일이 생길 수 있다. 따라서 사전에 충분한 파티션을 확보하여 프로덕션 환경에서 파티션의 수가 동적으로 늘어나는 일은 피하는 것이 좋다.  
- 토픽의 물리적 샤딩: 지역에 따라 여러 토픽을 둘 수도 있고, 사업 유형에 따라 둘 수도 있을 것이다.
  - 장점: 데이터를 여러 토픽으로 나누면 시스템의 처리 대역폭을 높일 수 있다.
  - 단점: 복잡성이 증가하고 유지 관리 비용이 늘어난다.

#### 집계 서비스의 규모 확장

- 방안 #1: ad_id마다 별도의 처리 스레드를 두는 방안
- 방안 #2: 집계 서비스 노드를 아파치 하둡 YARN 같은 자원 공급자에 배포하는 방식이다.

방안 #1은 구현이 쉽다.  
방안 #2가 더 많은 컴퓨팅 자원을 추가하여 시스템 규모를 확장할 수 있어서 많이 쓰인다.  

> Hadoop YARN의 구조와 동작 방식  
> https://mangkyu.tistory.com/127  

#### 데이터베이스의 규모 확장

카산드라는 안정 해시와 유사한 방식으로 수평적인 규모 확장을 기본적으로 지원하고 있다.  

#### 핫스팟 문제

자원 관리자에 추가 자원을 신청한다.  
집계 서비스 노드는 각 서비스 노드가 100개씩 이벤트를 처리할 수 있도록 이벤트를 세 개 그룹으로 분할한다.  

### 결함 내성

집계는 메모리에서 이루어지므로 집계 노드에 장애가 생기면 집계 결과도 손실된다.  
카프카 데이터를 원점부터 다시 재생하여 집계하면 시간이 오래 걸린다.  
그러니 업스트림 오프셋 같은 '시스템 상태'를 스냅숏으로 저장하고 마지막으로 저장된 상태부터 복구해 나가는 것이 바람직하다.  

## 데이터 모니터링 및 정확성

### 지속적 모니터링

- 지연 시간
- 메시지 큐 크기
- 집계 노드의 시스템 자원

### 조정(대사)

조정은 다양한 데이터를 비교하여 데이터 무결성을 보증하는 기법을 일컫는다.  

> 수집된 지표를 광고주의 랜딩 페이지뷰와 비교하거나 클릭 이벤트 트랙킹 등으로 비교했었음  

### 대안적 설계안

광고 클릭 데이터를 하이브에 저장한 다음 빠른 질의는 ElasticSearch 계층을 얹어서 처리하는 것이다.  
집계는 클릭하우스나 드루이드 같은 OLAP 데이터베이스를 통해 처리할 수 있을 것이다.  


## 4단계: 마무리

> 클릭 어뷰징 검증  
> 리다이렉트 방식 (http status code)  
